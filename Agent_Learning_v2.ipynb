{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IntelliGoal(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(s):\n",
    "        super(IntelliGoal, s).__init__()\n",
    "        \n",
    "        s.screen_width = 1240\n",
    "        s.screen_height = 800\n",
    "        s.border_width = 10\n",
    "        s.screen = None\n",
    "        \n",
    "        s.goal_radius = 20\n",
    "        s.player_radius = 10\n",
    "        \n",
    "        s.player_speed = 1\n",
    "        \n",
    "        \n",
    "        # State information\n",
    "        s.player_x = 0\n",
    "        s.player_y = 0\n",
    "        s.goal_x = 0\n",
    "        s.goal_y = 0\n",
    "        s.state = np.array([s.player_x, s.player_y, s.goal_x, s.goal_y])\n",
    "        s.prev_distance = math.sqrt((s.player_x - s.goal_x) ** 2 + (s.player_y - s.goal_y) ** 2)\n",
    "        s.reward = 0\n",
    "        \n",
    "        s.done = False\n",
    "        s.trucated = False\n",
    "        s.truncation_step_limit = 300\n",
    "        s.truncation_step_counter = 0\n",
    "        \n",
    "        \n",
    "        # State and Action spaces\n",
    "        s.action_space = spaces.Discrete(5) # 0: No movement, 1: Up, 2: Down, 3: Left, 4: Right\n",
    "        # observation space = [player_x, player_y, goal_x, goal_y]\n",
    "        s.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0]), \n",
    "            high=np.array([s.screen_width, s.screen_height, s.screen_width, s.screen_height]), \n",
    "            dtype=np.int32\n",
    "        )\n",
    "    \n",
    "    def seed(s, seed=None):\n",
    "        if seed is not None and (seed < 0 or seed >= 2**32):\n",
    "            raise ValueError(\"Seed must be between 0 and 2**32 - 1\")\n",
    "        s.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "        \n",
    "    def reset(s, seed=0):\n",
    "        s.seed(seed)\n",
    "        \n",
    "        s.player_x = s.screen_width // 2\n",
    "        s.player_y = s.screen_height // 2\n",
    "        \n",
    "        s.goal_x, s.goal_y = s.respawn_goal()\n",
    "        \n",
    "        s.reward = 0\n",
    "        s.state = np.array([s.player_x, s.player_y, s.goal_x, s.goal_y])\n",
    "        \n",
    "        \n",
    "        s.done = False\n",
    "        s.trucated = False\n",
    "        s.truncation_step_counter = 0\n",
    "        \n",
    "        return s.state, {}\n",
    "    \n",
    "    def respawn_goal(s):\n",
    "        s.goal_x = s.player_x + random.randint(-50, 50)\n",
    "        s.goal_y = s.player_y + random.randint(-50, 50)\n",
    "        \n",
    "        # clip the goal to the screen\n",
    "        offset = s.border_width + s.goal_radius\n",
    "        s.goal_x = min( max(s.goal_x, offset), s.screen_width - offset)\n",
    "        s.goal_y = min( max(s.goal_y, offset), s.screen_height - offset)\n",
    "        \n",
    "        pygame.time.wait(300)\n",
    "        \n",
    "        s.prev_distance = math.sqrt((s.player_x - s.goal_x) ** 2 + (s.player_y - s.goal_y) ** 2)\n",
    "        \n",
    "        return s.goal_x, s.goal_y\n",
    "        \n",
    "    def step(s, action):\n",
    "        #################\n",
    "        #do more here?\n",
    "        #################\n",
    "        \n",
    "        s.do_action(action)\n",
    "        s.calculate_reward()\n",
    "        \n",
    "        s.truncation_step_counter += 1\n",
    "        if s.truncation_step_counter >= s.truncation_step_limit:\n",
    "            s.reward -= 0.4\n",
    "            s.trucated = True\n",
    "\n",
    "        if s.reward < -1:\n",
    "            s.reward = -1\n",
    "            s.trucated = False\n",
    "        \n",
    "        if s.reward > 1:\n",
    "            s.reward = 1\n",
    "        \n",
    "        \n",
    "        return s.state, s.reward, s.done, s.trucated, {}\n",
    "        \n",
    "   \n",
    "    def do_action(s, action):\n",
    "        if not s.action_space.contains(action):\n",
    "            print(\"Invalid Action\")\n",
    "        \n",
    "        s.prev_distance = math.sqrt((s.player_x - s.goal_x) ** 2 + (s.player_y - s.goal_y) ** 2)\n",
    "        \n",
    "        s.player_x += (action == 4)*s.player_speed - (action == 3)*s.player_speed \n",
    "        s.player_y += (action == 2)*s.player_speed - (action == 1)*s.player_speed\n",
    "        \n",
    "    def calculate_reward(s):\n",
    "        # Distance between player and goal\n",
    "        distance = math.sqrt((s.player_x - s.goal_x) ** 2 + (s.player_y - s.goal_y) ** 2)\n",
    "\n",
    "        # Check if the red dot touches the border\n",
    "        if s.player_x not in range(s.border_width, s.screen_width - s.border_width) or s.player_y not in range(s.border_width, s.screen_height - s.border_width):\n",
    "            s.player_x, s.player_y = s.screen_width // 2, s.screen_height // 2\n",
    "\n",
    "        # Check if the goal is reached within a certain range\n",
    "        if distance < s.goal_radius + s.player_radius:\n",
    "            s.goal_x, s.goal_y = s.respawn_goal()\n",
    "            s.reward += 1\n",
    "            s.done = True  \n",
    "    \n",
    "        s.state = np.array([s.player_x, s.player_y, s.goal_x, s.goal_y])\n",
    "        \n",
    "        # Reward the agent for getting closer to the goal\n",
    "        if s.prev_distance > distance:\n",
    "            s.reward += 0.010\n",
    "        if s.prev_distance < distance:\n",
    "            s.reward -= 0.015\n",
    "        \n",
    "        # Penalize the agent for taking too long\n",
    "        s.reward -= 0.0001\n",
    "     \n",
    "    \n",
    "    def keyboard_input(s):\n",
    "        keys = pygame.key.get_pressed()\n",
    "        # 1: Up, 2: Down, 3: Left, 4: Right\n",
    "        if keys[pygame.K_UP]:\n",
    "            return 1\n",
    "        if keys[pygame.K_DOWN]:\n",
    "            return 2\n",
    "        if keys[pygame.K_LEFT]:\n",
    "            return 3\n",
    "        if keys[pygame.K_RIGHT]:\n",
    "            return 4\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    def render(s, mode = 'human'):\n",
    "        if s.screen == None:\n",
    "            pygame.init()\n",
    "            s.screen = pygame.display.set_mode((s.screen_width, s.screen_height))\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "        s.player_x, s.player_y, s.goal_x, s.goal_y = s.state\n",
    "        \n",
    "        s.screen.fill((0, 0, 0))\n",
    "        RED = (255, 0, 0)\n",
    "        BLUE = (0, 0, 255)\n",
    "        BORDER_COLOR = (255, 255, 0)\n",
    "        \n",
    "        # Draw the border\n",
    "        pygame.draw.rect(s.screen, BORDER_COLOR, (0, 0, s.screen_width, s.border_width))\n",
    "        pygame.draw.rect(s.screen, BORDER_COLOR, (0, 0, s.border_width, s.screen_height))\n",
    "        pygame.draw.rect(s.screen, BORDER_COLOR, (0, s.screen_height - s.border_width, s.screen_width, s.border_width))\n",
    "        pygame.draw.rect(s.screen, BORDER_COLOR, (s.screen_width - s.border_width, 0, s.border_width, s.screen_height))\n",
    "\n",
    "        # Draw the red dot\n",
    "        pygame.draw.circle(s.screen, RED, (s.player_x, s.player_y), 10)\n",
    "\n",
    "        # Draw the blue dot\n",
    "        pygame.draw.circle(s.screen, BLUE, (s.goal_x, s.goal_y), s.goal_radius)\n",
    "        \n",
    "        #Display Reward on pygame screen\n",
    "        font = pygame.font.Font(None, 36)\n",
    "        text = font.render(\"Reward: \" + str(round(s.reward, 5)), True, (255, 255, 255))\n",
    "        s.screen.blit(text, (10, 10))\n",
    "        \n",
    "        pygame.display.update()\n",
    "    \n",
    "    def close(s):\n",
    "        pygame.quit()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Env with keyboard input actions\n",
    "test_env = IntelliGoal()\n",
    "test_env.reset(0)\n",
    "done = False\n",
    "i = 1\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         test_env.render()\n",
    "#         action = test_env.keyboard_input()\n",
    "#         state, reward, done, truncated, _i = test_env.step(action)\n",
    "#         if done or truncated:\n",
    "#             print(\"Reward at step - \", i, \" : \", round(reward, 5))\n",
    "#             i += 1\n",
    "#             test_env.reset(0)\n",
    "#     except:\n",
    "#         print(\"Environment Closed\")\n",
    "#         break\n",
    "\n",
    "# while True:\n",
    "#     test_env.render()\n",
    "#     action = test_env.keyboard_input()\n",
    "#     state, reward, done, truncated, _i = test_env.step(action)\n",
    "#     if done or truncated:\n",
    "#         print(\"Reward at step - \", i, \" : \", round(reward, 5))\n",
    "#         i += 1\n",
    "#         test_env.reset(0)   \n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU -  NVIDIA RTX A500 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Using GPU - \", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = IntelliGoal()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "def make_env():\n",
    "    env = IntelliGoal()\n",
    "    return env\n",
    "\n",
    "num_envs = 4\n",
    "take_logs = False\n",
    "\n",
    "if take_logs:\n",
    "    log_dir = \"./logs/ppo_intelligoal_a3_explore-more\"\n",
    "else:\n",
    "    log_dir = None\n",
    "      \n",
    "if num_envs == 1:\n",
    "    env = DummyVecEnv([make_env] * num_envs)\n",
    "else:\n",
    "    env = SubprocVecEnv([make_env] * num_envs)\n",
    "\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1, device=device)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            ent_coef=0.01,\n",
    "            clip_range=0.3,\n",
    "            sde_sample_freq=10,\n",
    "            learning_rate=0.0004,\n",
    "            tensorboard_log=log_dir, \n",
    "            device=\"cuda\",\n",
    "            n_steps=2048*4,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = \"./models/intelli_goal_a3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 849   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 601        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 109        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02457227 |\n",
      "|    clip_fraction        | 0.0359     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | 0.00256    |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | 6.01       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00244   |\n",
      "|    value_loss           | 11.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 547         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014646885 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.164      |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 4.75        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 8.83e-05    |\n",
      "|    value_loss           | 8.91        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019538851 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | -0.026      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 512         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014484845 |\n",
      "|    clip_fraction        | 0.0231      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.00471    |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 4.31        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000402   |\n",
      "|    value_loss           | 11.8        |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 505      |\n",
      "|    iterations           | 6        |\n",
      "|    time_elapsed         | 388      |\n",
      "|    total_timesteps      | 196608   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.01323  |\n",
      "|    clip_fraction        | 0.0326   |\n",
      "|    clip_range           | 0.3      |\n",
      "|    entropy_loss         | -1.59    |\n",
      "|    explained_variance   | 0.148    |\n",
      "|    learning_rate        | 0.0004   |\n",
      "|    loss                 | -0.0303  |\n",
      "|    n_updates            | 50       |\n",
      "|    policy_gradient_loss | -0.00159 |\n",
      "|    value_loss           | 1.23     |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 505         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 454         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020509586 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 0.471       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 509         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 514         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016386757 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.0004      |\n",
      "|    loss                 | 0.000996    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.000804   |\n",
      "|    value_loss           | 0.238       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 515        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 572        |\n",
      "|    total_timesteps      | 294912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02417858 |\n",
      "|    clip_fraction        | 0.0635     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | 0.853      |\n",
      "|    learning_rate        | 0.0004     |\n",
      "|    loss                 | -0.0476    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | 0.000829   |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "Training Interrupted\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "# save_path = \"./models/intelli_goal_a2\"\n",
    "# # set teh goal to spawn close - +_ 50 pixels initially\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    #append number to the file name\n",
    "    i = save_path[-1]\n",
    "    print(i)\n",
    "    i = int(i) + 1\n",
    "    save_path = save_path[:-1] + str(i)\n",
    "    print(\"Model already exists. Saving new one as - \", save_path)\n",
    "    \n",
    "try:\n",
    "    model.learn(total_timesteps=9999999999, reset_num_timesteps=False, progress_bar=False)\n",
    "    model.save(save_path)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training Interrupted\")\n",
    "    model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must install tqdm and rich in order to use the progress bar callback. It is included if you install stable-baselines with the extra packages: `pip install stable-baselines3[extra]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mload(save_path)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9999999999999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(save_path)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:287\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    278\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    285\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 287\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\base_class.py:434\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mconfigure_logger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensorboard_log, tb_log_name, reset_num_timesteps)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Create eval callback if needed\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_timesteps, callback\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\base_class.py:378\u001b[0m, in \u001b[0;36mBaseAlgorithm._init_callback\u001b[1;34m(self, callback, progress_bar)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# Add progress bar callback\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar:\n\u001b[1;32m--> 378\u001b[0m     callback \u001b[38;5;241m=\u001b[39m CallbackList([callback, \u001b[43mProgressBarCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[0;32m    380\u001b[0m callback\u001b[38;5;241m.\u001b[39minit_callback(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m callback\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\callbacks.py:690\u001b[0m, in \u001b[0;36mProgressBarCallback.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tqdm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    691\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install tqdm and rich in order to use the progress bar callback. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    692\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is included if you install stable-baselines with the extra packages: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install stable-baselines3[extra]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    694\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: You must install tqdm and rich in order to use the progress bar callback. It is included if you install stable-baselines with the extra packages: `pip install stable-baselines3[extra]`"
     ]
    }
   ],
   "source": [
    "# Load and resume training\n",
    "try:\n",
    "    model.load(save_path)\n",
    "    model.learn(total_timesteps=9999999999999, reset_num_timesteps=False, progress_bar=True)\n",
    "    model.save(save_path)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training Interrupted\")\n",
    "    print(\"Saving Model\")\n",
    "    model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 0.48489999771118164, Std Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=1)\n",
    "print(f\"Mean Reward: {mean_reward}, Std Reward: {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Wrap your environment in a DummyVecEnv for compatibility\u001b[39;00m\n\u001b[0;32m      9\u001b[0m env \u001b[38;5;241m=\u001b[39m DummyVecEnv([make_env()])\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(save_path, env\u001b[38;5;241m=\u001b[39menv, device\u001b[38;5;241m=\u001b[39m\u001b[43mdevice\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_model\u001b[39m():\n\u001b[0;32m     14\u001b[0m     obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# This will now be in the correct batch format\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = IntelliGoal()\n",
    "        env.render_mode = 'human'\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Wrap your environment in a DummyVecEnv for compatibility\n",
    "env = DummyVecEnv([make_env()])\n",
    "\n",
    "model = PPO.load(save_path, env=env, device=device)\n",
    "\n",
    "def run_model():\n",
    "    obs = env.reset()  # This will now be in the correct batch format\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "\n",
    "try:\n",
    "    run_model()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Environment Closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
